{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fdd3251-3196-4656-80c4-850b7fecac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39591e3f-3207-4b69-b0b2-6942d0c47fcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1506a3e6-e832-4026-aafc-35e3560cb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_PATH = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b149ee-6404-4930-9aef-46626e4c3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e97c37-a5e3-48f4-9b65-a480272e11a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDFs.\n",
      "Processing AttentionIsAllYouNeed.pdf\n",
      "Loaded 15 pages added\n",
      "Processing RadGraph.pdf\n",
      "Loaded 12 pages added\n",
      "\n",
      "Total documents loaded 27\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\" Process all PDFs \"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    # Find all PDFs recursively\n",
    "    pdf_files = list(\n",
    "        pdf_dir.glob(\"**/*.pdf\") # Match any PDF, no matter now far in the subdirectories\n",
    "    )\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDFs.\")\n",
    "\n",
    "    # So each pdf_file is a path to that PDF file\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"Processing {pdf_file.name}\")\n",
    "        try:\n",
    "            # Load that pdf file\n",
    "            # PyPDFLoader creates chunks from each PDF\n",
    "            # These chunks are Document objects\n",
    "            # By default PyPDFLoader will split the PDF as a single text flow\n",
    "            # This means that they are chunked by one continouos page\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages added\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error {e}\")\n",
    "\n",
    "    print(f\"\\nTotal documents loaded {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(DATA_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7923d55-6459-4f14-8100-8abcb69d843b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='1 Introduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 · d) O(1) O(1)\\nRecurrent O(n · d2) O(n) O(n)\\nConvolutional O(k · n · d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r · n · d) O(1) O(n/r)\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nP E(pos,2i) = sin(pos/100002i/dmodel )\\nP E(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\\nP Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < ndoes not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [18] 23.75\\nDeep-Att + PosUnk [39] 39.2 1.0 · 1020\\nGNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020\\nConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020\\nMoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020\\nGNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021\\nConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.8 2.3 · 1019\\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3 English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser Training WSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\\nTransformer (4 layers) WSJ only, discriminative 91.3\\nZhu et al. (2013) [40] semi-supervised 91.3\\nHuang & Harper (2009) [14] semi-supervised 91.3\\nMcClosky et al. (2006) [26] semi-supervised 92.1\\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\\nTransformer (4 layers) semi-supervised 92.7\\nLuong et al. (2015) [23] multi-task 93.0\\nDyer et al. (2016) [8] generative 93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7 Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 0, 'page_label': '1', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='RadGraph: Extracting Clinical Entities and Relations\\nfrom Radiology Reports\\nSaahil Jain∗\\nStanford University\\nsaahil.jain@cs.stanford.edu\\nAshwin Agrawal∗\\nStanford University\\nashwin15@stanford.edu\\nAdriel Saporta∗\\nStanford University\\nasaporta@cs.stanford.edu\\nSteven QH Truong\\nVinBrain, VinUniversity\\nDu Nguyen Duong\\nVinBrain\\nTan Bui\\nVinBrain\\nPierre Chambon\\nStanford University\\nYuhao Zhang\\nStanford University\\nMatthew P. Lungren\\nStanford University\\nAndrew Y. Ng\\nStanford University\\nCurtis P. Langlotz†\\nStanford University\\nlanglotz@stanford.edu\\nPranav Rajpurkar†\\nHarvard University\\npranav_rajpurkar@hms.harvard.edu\\nAbstract\\nExtracting structured clinical information from free-text radiology reports can\\nenable the use of radiology report information for a variety of critical healthcare\\napplications. In our work, we present RadGraph, a dataset of entities and relations\\nin full-text chest X-ray radiology reports based on a novel information extraction\\nschema we designed to structure radiology reports. We release a development\\ndataset, which contains board-certiﬁed radiologist annotations for 500 radiology\\nreports from the MIMIC-CXR dataset (14,579 entities and 10,889 relations), and\\na test dataset, which contains two independent sets of board-certiﬁed radiologist\\nannotations for 100 radiology reports split equally across the MIMIC-CXR and\\nCheXpert datasets. Using these datasets, we train and test a deep learning model,\\nRadGraph Benchmark, that achieves a micro F1 of 0.82 and 0.73 on relation ex-\\ntraction on the MIMIC-CXR and CheXpert test sets respectively. Additionally, we\\nrelease an inference dataset, which contains annotations automatically generated\\nby RadGraph Benchmark across 220,763 MIMIC-CXR reports (around 6 million\\nentities and 4 million relations) and 500 CheXpert reports (13,783 entities and\\n9,908 relations) with mappings to associated chest radiographs. Our freely avail-\\nable dataset can facilitate a wide range of research in medical natural language\\nprocessing, as well as computer vision and multi-modal learning when linked to\\nchest radiographs.\\n1 Introduction\\nRadiology reports are comprised of free text containing critical information about a patient’s health\\nbased on an interpretation of radiology images and clinical history. However, their unstructured\\nnature combined with the complexity and ambiguity of natural language pose a challenge when using\\n∗Equal Contribution\\n†Equal Contribution\\n35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.\\narXiv:2106.14463v3  [cs.CL]  29 Aug 2021'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 1, 'page_label': '2', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='Figure 1: Process for developing RadGraph, our dataset of annotated entities and relations in radiology\\nreports. First, board-certiﬁed radiologists annotate reports for our development and test datasets.\\nSecond, we train a deep learning model using the development dataset. Third, we use our model to\\nautomatically generate annotations for a much larger number of MIMIC-CXR and CheXpert reports,\\nwhich can be mapped to associated chest radiographs.\\nradiology reports for clinical research and other downstream applications, especially in settings with\\nlimited labeled data. Automatically extracting clinically relevant information from radiology reports\\ncan enable various use cases, ranging from large-scale training of medical imaging models to disease\\nsurveillance.\\nNumerous approaches for extracting information from radiology reports have been developed with\\nvarying intended use cases. Large-scale datasets, such as MIMIC-CXR [1] and CheXpert [2], use\\nautomated radiology report labelers [2–6] to extract common medical conditions from reports. Other\\napproaches [7–11] aim to extract more ﬁne-grained information in reports. The development of\\nautomated approaches for structuring large amounts of clinically relevant information in reports is\\nprimarily limited by two factors. First, the choice of information extraction schema, such as the 14\\nmedical conditions proposed by Irvin et al. [ 2], limits the amount of information extracted from\\nreports. Second, there is a limited number of datasets with dense report annotations, which are\\nexpensive to obtain given the amount of time and expertise required by medical experts to procure\\nsuch annotations.\\nIn our work, we seek to address these limitations by creating RadGraph, a dataset of clinical entity\\nand relation annotations for radiology reports. Our four primary contributions are the following.\\n(1) We deﬁne a novel information extraction schema for radiology reports, intended to cover most\\nclinically relevant information within the report while allowing for ease and consistency during\\nannotation. (2) We release development and test datasets annotated according to our schema by board-\\ncertiﬁed radiologists. Our development dataset contains annotations for 500 radiology reports from\\nthe MIMIC-CXR dataset, consisting of 14,579 entities and 10,889 relations. Our test dataset contains\\ntwo sets of independent annotations for 100 radiology reports from the MIMIC-CXR and CheXpert\\ndatasets. (3) We use our dataset to benchmark various modeling approaches. Our best approach,\\nwhich we call RadGraph Benchmark, achieves a micro F1 of 0.94/0.91 (MIMIC-CXR/CheXpert) on\\nnamed entity recognition and a micro F1 of 0.82/0.73 (MIMIC-CXR/CheXpert) on relation extraction.\\n(4) We release an inference dataset, which contains annotations automatically generated by RadGraph\\nBenchmark for 220,763 MIMIC-CXR reports, consisting of over 6 million entities and 4 million\\nrelations, and 500 CheXpert reports, consisting of 13,783 entities and 9,908 relations. Annotated\\nreports in the inference dataset have mappings to associated chest radiographs, which can facilitate\\nthe development of multi-modal approaches in radiology. We summarize our process for creating\\nRadGraph in Figure 1.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 2, 'page_label': '3', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='2 Related work\\nVarious natural language processing (NLP) approaches have been developed and used to extract\\ninformation from radiology reports. One approach uses automated radiology report labelers [2–6]\\nto label reports within large chest radiograph datasets, such as MIMIC-CXR [1] and CheXpert [2],\\nfor a restricted number of common medical conditions. However, these labels do not capture critical\\nﬁne-grained information contained in a radiology report, such as speciﬁc entities and their relations.\\nWhile analysis tools [12–14] have been developed to extract key clinical concepts and their attributes\\nfrom biomedical text and convert them into a structured format, such dictionary- and rule-based\\nannotation systems are often limited in their report coverage and generalizability across institutions.\\nAnother approach aims to capture more speciﬁc and detailed information from radiology reports by\\nadopting entity extraction schemas [8, 11] or schemas that focus on facts [10] and spatial relations\\n[9, 15]. A central limitation of this approach is that it requires task-speciﬁc datasets to be densely\\nannotated by domain experts.\\nTo address this need for more speciﬁc annotations for radiology reports, datasets have been developed\\nfor information extraction from radiology reports. RadCore [8] is a multi-institutional database of\\nradiology reports that contains entity-level annotations. PadChest [16] consists of chest radiographs\\nand reports labeled with 174 different radiographic ﬁndings, 19 differential diagnoses, and 104\\nanatomic locations. Datta et al. [ 17] released a dataset of radiology reports annotated according to a\\nschema based on spatial role labeling. Our dataset of radiology reports with dense annotations for\\nboth entities and relations extracts a broader range of information from the radiology text using a new\\ninformation extraction schema designed for report coverage and generalizability. Outside the domain\\nof radiology, several datasets have been speciﬁcally developed for the tasks of entity and relation\\nextraction, such as SciERC [18] and SemEval 2017 Task 10 [19] for scientiﬁc information extraction.\\nAlong with these datasets, there have been many advancements in NLP for the task of entity and\\nrelation extraction. Pipeline approaches [20, 21] decompose the task into separately trained subtasks\\n(named entity recognition [22, 23] and relation extraction [24]), while joint extraction approaches\\n[18, 25–27] model the two subtasks at the same time in order to capture interactions between entities\\nand relations. When developing benchmarks for our task, we use both a pipeline approach [20] and a\\njoint extraction approach [26].\\n3 Information extraction schema\\nFigure 2: Sample report annotated according to the RadGraph schema (left) and the associated\\nknowledge graph (right).\\nWe propose a novel information extraction schema for extracting entities and relations from radiology\\nreports, adapting the schema initially proposed by Langlotz et al. [7] to incorporate relations between\\nentities and reduce the number of entities. After iterating on the initial schema based on feedback\\nreceived from board-certiﬁed radiologists during labeling pilots, we design a schema that achieves\\ntwo goals. First, our schema is designed for high coverage of the clinically relevant information in a\\nreport corresponding to the radiology image being examined, generally included in the Findings and\\nImpression sections of the radiology report; we quantify the schema coverage in practice in Section\\n6.1. Second, our schema is designed to simplify the annotation task for radiologists, which improves\\nlabeling consistency and speed. Our schema is not designed to capture clinical context and history in\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 3, 'page_label': '4', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='other report sections, as they contain information that is better documented elsewhere in the patient\\nrecord and would require a more complex schema that would increase the difﬁculty of annotating.\\nEntities We deﬁne an entity as a continuous span of text that can include one or more adjacent\\nwords. Entities in our schema center around two concepts: Anatomy and Observation. We specify\\nthree uncertainty levels for Observation, so our schema deﬁnes four entities: Anatomy, Observation:\\nDeﬁnitely Present, Observation: Uncertain, and Observation: Deﬁnitely Absent. Anatomy refers\\nto an anatomical body part that occurs in a radiology report, such as a “lung”. Observations refer\\nto words associated with visual features, identiﬁable pathophysiologic processes, or diagnostic\\ndisease classiﬁcations. For example, an Observation could be “effusion” or more general phrases like\\n“increased”.\\nRelations We deﬁne a relation as a directed edge between two entities. Our schema uses three\\nrelations: Suggestive Of, Located At, and Modify. We select these three relations based on feedback\\nfrom board-certiﬁed radiologists during labeling pilots to sufﬁciently cover the clinically relevant\\nrelationships between our entities with a relatively small number of relation types. We deﬁne the\\npermitted use of each relation using the format: relation type (ﬁrst entity type, second entity type).\\nSuggestive Of (Observation, Observation) is a relation between two Observation entities indicating\\nthat the presence of the second Observation is inferred from that of the ﬁrst Observation. Located At\\n(Observation, Anatomy) is a relation between an Observation entity and an Anatomy entity indicating\\nthat the Observation is related to the Anatomy. While Located At often refers to location, it can also\\nbe used to describe other relations between an Observation and an Anatomy. Modify (Observation,\\nObservation) or (Anatomy, Anatomy) is a relation between two Observation entities or two Anatomy\\nentities indicating that the ﬁrst entity modiﬁes the scope of, or quantiﬁes the degree of, the second\\nentity. As a result, all Observation modiﬁers are annotated as Observation entities, and all Anatomy\\nmodiﬁers are annotated as Anatomy entities for simplicity. By using relations to indicate modiﬁcation\\ninstead of distinct entity types for modiﬁers, which was the approach proposed by Langlotz et al. [ 7]\\nand used by Hassanpour et al. [8], our schema can more precisely capture the modifying relationships\\nwhile also simplifying the task for annotators. The distinction between modiﬁers and entities is often\\nnot semantically relevant or clear without relations, such as when modiﬁers modify other modiﬁers.\\nFigure 2 contains an example of a report annotated according to our schema and the resulting graph.\\n4 Dataset\\nTo facilitate the development of models that can extract information from radiology reports according\\nto our schema, we release RadGraph, a dataset containing radiology reports along with annotated\\nentities and relations for each report. Our dataset includes development and test datasets, which were\\nannotated by board-certiﬁed radiologists, as well as an inference dataset, which was annotated by our\\nbenchmark deep learning model.\\n4.1 Development and test datasets\\nTable 1: Development and test dataset annotation statistics\\nTrain (%) Dev (%) Test - MIMIC-CXR (%) Test - CheXpert (%)\\nAnatomy 5,366 (43.3) 987 (45.0) 528 (40.8) 640 (43.4)\\nObservation: Deﬁnitely Present 5,046 (40.7) 831 (37.9) 478 (37.0) 603 (40.9)\\nObservation: Uncertain 587 (4.7) 93 (4.2) 43 (3.3) 55.5 (3.8)\\nObservation: Deﬁnitely Absent 1,389 (11.2) 280 (12.8) 244 (18.9) 175 (11.9)\\nTotal Entities 12,388 (100) 2,191 (100) 1,293 (100) 1,473.5 (100)\\nModify 5,641 (61.0) 1,013 (61.8) 522 (57.8) 696 (62.9)\\nLocated at 3,179 (34.4) 554 (33.8) 354 (39.2) 348 (31.5)\\nSuggestive of 431 (4.7) 71 (4.3) 26.5 (2.9) 62.5 (5.6)\\nTotal Relations 9,251 (100) 1,638 (100) 902.5 (100) 1,106.5 (100)\\nTo construct our development and test datasets, we obtained radiology report annotations according\\nto our schema from three board-certiﬁed radiologists, each with at least eight years of experience.\\nWe ran three labeling pilots, which included around 15 reports each, to train our radiologists and\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 4, 'page_label': '5', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='Table 2: Development, test, and inference dataset demographic statistics\\nAttribute Train\\n(%)\\nDev\\n(%)\\nTest\\nMIMIC-CXR (%)\\nTest\\nCheXpert (%)\\nInference\\nMIMIC-CXR (%)\\nInference\\nCheXpert (%)\\nSex Male 54.6 72.0 10.0 48.0 51.2 54.4\\nFemale 45.4 28.0 90.0 52.0 48.6 45.6\\nUnknown 0.0 0.0 0.0 0.0 0.2 0.0\\nAge 0-20 1.9 1.3 4.0 2.0 0.9 0.6\\n21-40 5.9 6.7 0.0 22.0 11.5 13.8\\n41-60 26.8 21.3 16.0 36.0 28.4 31.2\\n61-80 40.9 69.3 66.0 32.0 41.3 46.6\\n81+ 24.5 1.3 14.0 8.0 17.7 7.8\\nUnknown 0.0 0.0 0.0 0.0 0.2 0.0\\nRace White 65.4 56.0 32.0 46.0 61.2 41.8\\nBlack 16.0 20.0 62.0 4.0 15.2 4.6\\nHispanic 4.0 0.0 0.0 8.0 5.2 27.2\\nAsian 2.6 0.0 0.0 8.0 3.2 7.0\\nNative 0.5 0.0 0.0 0.0 0.3 1.4\\nOther 4.0 21.3 0.0 18.0 4.4 8.4\\nUnknown 7.5 2.7 6.0 16.0 10.4 9.6\\niteratively improved our schema based on their feedback. To ensure high quality annotations, we do\\nnot include any of the annotations obtained during pilot labeling initiatives in our released dataset.\\nThe radiologists used a text labeling platform (Datasaur.ai [28], Sunnyvale, CA) to directly annotate\\nthe free-text reports according to our schema. We provide a breakdown of our development and\\ntest datasets by entities, relations, and data splits in Table 1. Additionally, in Table 2 we provide\\ndemographic breakdowns by sex, age, and race for our development and test datasets, as well as for\\nour inference dataset, which we describe in Section 4.2.\\nDevelopment dataset We sample 500 radiology reports from the MIMIC-CXR dataset [1] for our\\ndevelopment dataset. Each report is annotated by a single board-certiﬁed radiologist, resulting in\\n14,579 entities and 10,889 relations across all reports. Our development dataset was divided into train\\nand dev sets, where the dev set includes 15% of the development dataset. Patients associated with\\nreports in the train set and dev set do not overlap.\\nTest dataset We sample 50 radiology reports from the MIMIC-CXR dataset and 50 radiology\\nreports from the CheXpert dataset [2] for our test dataset in order to test generalization of approaches\\nacross institutions. Each report is independently annotated by two board-certiﬁed radiologists,\\nresulting in an average of 2,766 entities and 2,009 relations per annotator across all reports. Patients\\nassociated with reports in the MIMIC-CXR test dataset and the development dataset do not overlap.\\n4.2 Inference dataset\\nFirst, we develop and measure the performance of a deep learning model called RadGraph Benchmark,\\nwhich we describe further in Section 5, using our development and test datasets respectively. Next, we\\nuse Radgraph Benchmark to automatically annotate 220,763 reports from the MIMIC-CXR dataset\\nand 500 reports from the CheXpert dataset.\\nOn our selected MIMIC-CXR reports, we annotate 6,161,934 entities (2,699,288 Anatomy, 2,386,057\\nObservation: Deﬁnitely Present, 273,301 Observation: Uncertain, 803,288 Observation: Deﬁnitely\\nAbsent) and 4,409,026 relations (2,683,628 Modify, 1,554,406 Located At, 170,992 Suggestive Of).\\nOn our selected CheXpert reports, we annotate 13,783 entities (5,825 Anatomy, 6,453 Observation:\\nDeﬁnitely Present, 515 Observation: Uncertain, 990 Observation: Deﬁnitely Absent) and 9,908\\nrelations (6,467 Modify, 3,084 Located At, 357 Suggestive Of).\\n4.3 De-identiﬁcation of reports\\nEach report in our dataset is de-identiﬁed according to the US Health Insurance Portability Act\\n(HIPAA). MIMIC-CXR reports have already been de-identiﬁed by Johnson et al. [1], who replaced\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 5, 'page_label': '6', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='protected health information (PHI) in reports with three consecutive underscores. We de-identify\\nCheXpert reports using an automated, transformer-based de-identiﬁcation algorithm followed by\\nmanual review of each report. PHI is replaced with fake PHI following a hiding-in-plain-sight (HIPS)\\n[29] approach. The de-identiﬁcation of the CheXpert reports was conﬁrmed by manual review.\\n4.4 Data usage and ethics\\nOur dataset with documentation and associated code is hosted and maintained on PhysioNet under\\nthe following license: PhysioNet Credentialed Health Data License 1.5.0. It can be accessed at the\\nfollowing link: https://doi.org/10.13026/hm87-5p47.\\nOur data can be used for various purposes in the healthcare domain. We highlight two use cases. One\\nuse case is to develop NLP models for entity and relation extraction in radiology using our develop-\\nment dataset. Another use case is to develop multi-modal models in radiology using our inference\\ndataset, which enables linkage of full-text radiology reports, knowledge graphs (entities/relations as\\nper our schema), and associated chest radiographs from the MIMIC-CXR and CheXpert datasets.\\nWe also release the checkpoint for our model, RadGraph Benchmark, which can automatically\\nannotate radiology reports. Models developed using our dataset can have clinical impact in various\\nways. Examples can range from population-level analysis using entities and relations automatically\\nextracted from radiology reports to AI-assisted diagnosis using medical imaging models that can\\nautomatically generate knowledge graphs from radiology images.\\nTo avoid any potential harm to patients, researchers training models on our datasets should take\\ninto account potential distribution shifts that may occur when they apply their models to other\\ndatasets with different patient populations, as discussed further in Section 6.2. As recommended by\\nSeyyed-Kalantari et al. [30], when deploying clinical models in practice, researchers should audit\\nperformance disparities across attributes, such as sex, age, and race, which we report in Table 2 for\\nour datasets.\\n4.5 Limitations\\nFirst, as mentioned in Section 3, our schema does not capture clinical context in radiology reports,\\nsuch as information included in the Comparison or History sections. Second, as discussed in Section\\n6, there exist ambiguous cases in radiology reports that can be difﬁcult to label according to our\\nschema. Third, our annotations are limited to chest X-ray radiology reports from the MIMIC-CXR\\nand CheXpert datasets, although the RadGraph schema is designed to annotate radiology reports\\nin general. Fourth, the reports for both the MIMIC-CXR and CheXpert datasets are collected from\\nhospitals only in the United States (Beth Israel Deaconess Medical Center in Boston, MA, and\\nStanford Hospital in Stanford, CA, respectively). Fifth, as discussed in Section 6, our test dataset\\nis independently labeled by two radiologists, with more inter-observer variability on the CheXpert\\ntest set than on the MIMIC-CXR test set. Sixth, our datasets are not consistently balanced across\\ndemographic attributes. We report demographic statistics across all reports in our development, test,\\nand inference datasets in Table 2.\\n5 Benchmarks\\n5.1 Approaches\\nWe propose an entity and relation extraction task for radiology reports that can be developed using\\nour development dataset and tested using our test dataset. To support the development of methods\\nfor our task, our dataset processes each radiology report into a sequence of space-delimited tokens,\\nwhere punctuation like commas and semicolons have been separated from words to support entity\\nrecognition. For each report, we provide annotations identifying the type and span of each entity\\nas well as relations between entities. We describe several initial approaches to entity and relation\\nextraction for our task below.\\nBaseline model We develop a simple, transformer-based Baseline approach. Our Baseline approach\\nto entity and relation extraction uses a BERT [31] model with a linear classiﬁcation head on top of the\\nlast layer for NER and R-BERT [32] for relation extraction. For our baseline NER approach, since\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 6, 'page_label': '7', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='Table 3: Performance on relation extraction by approach\\nMIMIC-CXR CheXpert\\nMicro F1 Macro F1 Micro F1 Macro F1\\nRadiologist Benchmark 0.947 0.910 0.745 0.704\\nBaseline\\nBERT 0.468 0.372 0.424 0.356\\nBioBERT 0.507 0.412 0.451 0.387\\nBio+Clinical BERT 0.454 0.367 0.389 0.343\\nPubMedBERT 0.436 0.356 0.385 0.335\\nBlueBERT 0.428 0.339 0.341 0.282\\nDYGIE++\\nBERT 0.805 0.752 0.712 0.688\\nBioBERT 0.801 0.731 0.701 0.668\\nBio+Clinical BERT 0.806 0.739 0.701 0.672\\nPubMedBERT 0.823 0.783 0.725 0.692\\nBlueBERT 0.803 0.712 0.705 0.664\\nPURE\\nBERT 0.805 0.731 0.722 0.648\\nBioBERT 0.806 0.757 0.721 0.654\\nBio+Clinical BERT 0.809 0.746 0.728 0.664\\nPubMedBERT 0.812 0.745 0.729 0.679\\nBlueBERT 0.818 0.738 0.699 0.655\\nthe same entity may span multiple tokens, we use the IOB tagging scheme [33] and convert IOB tags\\nto entity types deﬁned by our schema after inference. We use a learning rate of 2e-5 (tuning range\\n2e-4 to 2e-6) with a batch size of 8 for our BERT-base model for NER after hyper-parameter tuning,\\nconsistent with the tuning approach used by Devlin et al. [ 31], and a learning rate of 2e-5 with a\\nbatch size of 4 with 16 gradient accumulation steps for our R-BERT model for relation extraction,\\nconsistent with the approach used by Wu et al. [32] except we use a smaller batch size.\\nBenchmark models We develop additional benchmark approaches for our task, using two different\\nentity and relation extraction model architectures. Our ﬁrst approach uses the DYGIE++ framework\\nby Wadden et al. [26], which achieved state-of-the-art at the time on NER and relation extraction by\\njointly extracting entities and relations. Our second approach uses the Princeton University Relation\\nExtraction system (PURE) by Zhong et al. [34], which achieved state-of-the-art at the time on relation\\nextraction using a pipeline approach that decomposes NER and relation extraction into separate\\nsubtasks.\\nWe use BERT in both our PURE and DYGIE++ approaches. For PURE NER, we select a learning\\nrate of 1e-5 (tuning range 1e-4 to 1e-6) with a batch size of 16 for BERT and a learning rate of 5e-5\\n(tuning range 5e-3 to 5e-5) for task speciﬁc layers. For PURE relation extraction, we use a learning\\nrate of 2e-5 (tuning range 2e-4 to 2e-6) with a batch size of 16 for BERT. For PURE NER, we use a\\nspan length of three for the PURE approach since only 0.3% of entities in our development dataset\\nconsist of more than three words. For PURE relation extraction, we use a context window of 50\\nwords based on average sentence length in our train set. For DYGIE++, we use a learning rate of\\n5e-5 with a batch size of 1 for BERT and a learning rate of 1e-3 for task speciﬁc layers, consistent\\nwith the approach used by Wadden et al. [26].\\nBiomedical pretraining For each of our approaches, in addition to using BERT weight initial-\\nizations, we use weight initializations from four different biomedical pretrained models, which are\\nBioBERT [35], Bio+ClinicalBERT [36], PubMedBERT [37], and BlueBERT [38].\\nTraining details We train our models using NVIDIA GeForce GTX 1080 GPUs (1 for Baseline, 1\\nfor DYGIE++, and 3 for PURE) until convergence, once for each approach before testing. Time to\\nconvergence varies across approaches, taking ∼ 1 hour for Baseline NER, ∼ 5 hours for Baseline\\nrelation extraction, ∼ 2 hours for DYGIE++ (joint NER and relation extraction), ∼ 1 hour for PURE\\nNER, and ∼ 10 hours for PURE relation extraction.\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 7, 'page_label': '8', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='Table 4: Benchmark performance on entity recognition\\nAnatomy Observation:\\nDeﬁnitely Present\\nObservation:\\nUncertain\\nObservation:\\nDeﬁnitely AbsentMicro F1 Macro F1\\nMIMIC-CXR\\nRadiologist Benchmark 0.994 0.981 0.953 0.996 0.988 0.981\\nRadGraph Benchmark 0.968 0.922 0.700 0.952 0.940 0.886\\nCheXpert\\nRadiologist Benchmark 0.944 0.917 0.757 0.960 0.928 0.894\\nRadGraph Benchmark 0.941 0.884 0.714 0.910 0.905 0.862\\nTable 5: Benchmark performance on relation extraction\\nModify Located At Suggestive Of Micro F1 Macro F1\\nMIMIC-CXR\\nRadiologist Benchmark 0.952 0.949 0.830 0.947 0.910\\nRadGraph Benchmark 0.804 0.861 0.685 0.823 0.783\\nCheXpert\\nRadiologist Benchmark 0.741 0.779 0.592 0.745 0.704\\nRadGraph Benchmark 0.709 0.779 0.588 0.725 0.692\\n5.2 Evaluation metrics\\nWe report both micro and macro F1 for entity recognition and relation extraction. For entity recogni-\\ntion, a predicted entity is considered correct if the predicted span boundaries and predicted entity type\\nare both correct. For relation extraction, a predicted relation is considered correct if the predicted\\nentity pair is correct (both the span boundaries and entity type) and the relation type is correct. For a\\nradiologist benchmark, we compute metrics using the two independent sets of radiologist annotations\\ncollected on our test set. For modeling approaches, we compute two metrics, where each metric uses\\nannotations acquired by a different labeler as ground truth, and then average the metrics. We report\\nresults on the MIMIC-CXR and CheXpert test sets separately.\\n5.3 Results\\nFirst, we compare the performance of each approach developed using our development dataset\\nalongside our radiologist benchmark on both the MIMIC-CXR and CheXpert test sets. When\\ncomparing approaches, we use the strict relation extraction metric deﬁned above as the primary\\nend-to-end approach metric, as it uses both predicted entities and relations in its computation. Both\\nDYGIE++ and PURE approaches obtain higher micro and macro F1 scores than the Baseline approach\\nbut lower micro and macro F1 scores than the radiologist benchmark on both test sets. We ﬁnd that\\nthe DYGIE++ approach with PubMedBERT initializations achieves the highest micro and macro\\nF1 scores on the MIMIC-CXR test set and the highest macro F1 score on the CheXpert test set for\\nrelation extraction. Accordingly, we call this approach the RadGraph Benchmark. We report our\\nresults for all approaches in Table 3.\\nSecond, we speciﬁcally evaluate the performance of RadGraph Benchmark alongside the radiologist\\nbenchmark on both entity recognition and relation extraction. We report metrics on the MIMIC-CXR\\ntest set followed by the CheXpert test set for RadGraph Benchmark and the human benchmark\\nas follows. RadGraph Benchmark achieves a micro F1 of 0.94/0.91 on named entity recognition\\nand a micro F1 of 0.82/0.73 on relation extraction. The human benchmark achieves a micro F1 of\\n0.99/0.93 on named entity recognition and a micro F1 of 0.95/0.75 on relation extraction. Both\\nRadGraph Benchmark and the human benchmark obtain lower micro F1 scores on the CheXpert test\\nset compared to the MIMIC-CXR test set for both named entity recognition and relation extraction.\\nFor entity recognition, both benchmarks obtain the highest F1 scores on Anatomy and Observation:\\nDeﬁnitely Absent and the lowest F1 score on Observation: Uncertain for both test sets. For relation\\nextraction, both benchmarks obtain the lowest F1 scores on Suggestive Of for both test sets. We\\nreport the full results for the benchmarks across entity types in Table 4 and across relation types in\\nTable 5.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 8, 'page_label': '9', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='Table 6: Schema coverage\\nAverage per report Development Test - MIMIC-CXR Test - CheXpert\\nSentences 6.6 6.1 8.2\\nSentences annotated 5.8 5.6 5.8\\nSentences annotated (%) 87.7% 92.3% 70.7%\\nTokens 62.0 52.7 61.7\\nTokens annotated 28.7 25.6 31.3\\nTokens annotated (%) 46.4% 48.6% 50.8%\\n6 Analysis\\n6.1 Schema coverage\\nGiven that existing information extraction systems for radiology reports often suffer from a lack of\\nreport coverage [8], we measure the number of tokens and sentences in report sections covered by our\\nschema. To calculate coverage, we extract the Findings and Impression sections of the reports, which\\nour schema is designed to annotate. We then calculate the average percent of sentences and tokens\\nannotated per report across the development and test datasets. For the token-level metrics, we exclude\\npunctuation. We ﬁnd that annotations obtained using our schema cover a high percentage of sentences\\nacross the development and test datasets. In the relevant sections of reports in the development\\ndataset, an average of 87.7% of sentences and 46.4% of tokens were annotated per report. The token\\nannotation percentage includes words that do not contain clinically relevant information, such as\\nstop words. Although the schema coverage in the MIMIC-CXR test set resembles the coverage of\\nthe development dataset, in the relevant sections of reports in the CheXpert test set, an average of\\n70.7% of sentences and 50.8% of tokens were annotated per report, suggesting that information in\\nCheXpert reports tends to be more concentrated in particular sentences. We report these results across\\nthe development and test sets in Table 6.\\n6.2 Annotation disagreements\\nWe explore disagreements that occur when annotating according to our schema. To measure agreement\\nbetween radiologists using our schema, we calculate Cohen’s Kappa [39] between the two annotators\\non each test set for the named entity recognition task and the relation extraction task separately. For\\nnamed entity recognition, we compute Kappa scores of 0.974 and 0.829 on the MIMIC-CXR and\\nCheXpert test sets respectively. For relation extraction, we compute Kappa scores of 0.841 and 0.397\\non the MIMIC-CXR and CheXpert test sets respectively. One reason for greater disagreement on the\\nCheXpert test set compared to the MIMIC-CXR test set may relate to different percentages of patients\\nin the intensive care unit (ICU) within the MIMIC-CXR dataset and the CheXpert dataset, which can\\nsystematically affect the contents of radiology reports. A second reason for greater disagreement may\\nresult from a higher concentration of annotations in a smaller number of sentences in the CheXpert\\ntest set, as reported in Section 6.1. Denser annotations can be more complicated to label, as they are\\nmore likely to contain layered relations that present ambiguities and difﬁculties described below.\\nWe ﬁnd ﬁve primary categories of inter-observer variability between annotators on our test dataset.\\nFirst, we observe disagreements related to the direction of relations. For example, given the phrase\\n“bands of coarse linear opacity,” one annotator annotated “bands,” “coarse,” and “linear” modifying\\n“opacity”, while the other annotated “bands” modifying “coarse”, “coarse” modifying “linear”, and\\n“linear” modifying “opacity”. Second, we observe disagreements annotating implantable devices such\\nas tubes, lines, and catheters. For example, for the phrase “pleural tube”, one annotator annotated\\n“pleural” as an Anatomy referencing the intended location of the tube. However, another annotator\\nannotated “pleural” as an Observation, where “pleural” indicates the type of tube. We consider the\\nsecond annotation to be correct, given that “pleural tube” does not indicate an anatomic location,\\nparticularly when misplaced. Third, given that we only provide three levels of uncertainty, we observe\\ndisagreements annotating certain Observations as Deﬁnitely Present or as Uncertain, such as “distal\\ntip” in the phrase “with distal tip not clearly seen”. Fourth, we observe disagreements related to\\nannotating a phrase as a single large entity or multiple smaller entities. For example, one radiologist\\nannotated “cephalad portion” as a single entity, while the other annotated it as two separate entities;\\nin general, we instruct radiologists to provide more granular annotations where possible. Fifth, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 9, 'page_label': '10', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='observe disagreements for challenging cases for which our schema does not mandate one correct\\nanswer, such as the phrase “right greater than left pleural effusions”.\\nNext, we explore disagreements between RadGraph Benchmark and radiologist annotators. We\\nﬁnd that the ﬁve areas of annotator disagreement described above likewise explain many of these\\ndisagreements. For example, while a radiologist annotator annotated the phrase “with suggestion\\nof osteopenia” as Observation: Deﬁnitely Present, the model annotated the phrase as Observation:\\nUncertain; this example falls under our third category, in which the level of uncertainty for an Obser-\\nvation may be ambiguous. Additionally, we ﬁnd that RadGraph Benchmark incorrectly disagrees\\nwith radiologist annotators when annotating rarer words in radiology reports, such as “fat pad”.\\n7 Conclusion\\nIn our work, we introduce RadGraph, a dataset of clinical entities and relations annotated in full-text\\nradiology reports using a novel information extraction schema for structuring radiology reports. First,\\nwe propose our schema, which is designed to extract clinically relevant information associated with a\\nradiologist’s interpretation of a medical image in a radiology report. Second, we release development\\nand test datasets annotated by board-certiﬁed radiologists. Each report is densely annotated, resulting\\nin 14,579 entities and 10,889 relations extracted from 500 reports in our development dataset. Third,\\nwe develop a deep learning model, called RadGraph Benchmark, which obtains a micro F1 of\\n0.82/0.73 (MIMIC-CXR/CheXpert) on relation extraction. Fourth, we release an inference dataset\\nannotated by our model for 220,763 MIMIC-CXR reports and 500 CheXpert reports, extracting\\naround 6 million entities/4 million relations and 13,783 entities/9,908 relations from each set of reports\\nrespectively. Each report in the inference dataset can be linked to its associated chest radiograph in\\nthe MIMIC-CXR or CheXpert dataset.\\nBy proposing a new schema and dataset for extracting clinically relevant information from un-\\nstructured text, we hope that RadGraph can facilitate a wide range of research in natural language\\nprocessing, as well as computer vision and multi-modal learning, in various medical domains.\\n8 Acknowledgements\\nWe would like to acknowledge Datasaur.ai for generously providing us access to their labeling\\nplatform. We would like to acknowledge Leo Anthony Celi and Tom Pollard from the MIMIC-CXR\\nteam and Nigam Shah, Ethan Chi, and Omar Khattab from Stanford University for their support.\\nResearch reported in this publication was made possible in part by the National Institute of Biomed-\\nical Imaging and Bioengineering (NIBIB) of the National Institutes of Health under contracts\\n75N92020C00008 and 75N92020C00021.\\nReferences\\n[1] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren,\\nChih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identiﬁed publicly available database\\nof chest radiographs with free-text reports. Scientiﬁc data, 6(1):1–8, 2019.\\n[2] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik\\nMarklund, Behzad Haghgoo, Robyn L. Ball, Katie S. Shpanskaya, Jayne Seekins, David A. Mong,\\nSafwan S. Halabi, Jesse K. Sandberg, Ricky Jones, David B. Larson, Curtis P. Langlotz, Bhavik N. Patel,\\nMatthew P. Lungren, and Andrew Y . Ng. Chexpert: A large chest radiograph dataset with uncertainty\\nlabels and expert comparison. CoRR, abs/1901.07031, 2019.\\n[3] Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers, and Zhiyong Lu. Negbio:\\na high-performance tool for negation and uncertainty detection in radiology reports. AMIA Summits on\\nTranslational Science Proceedings, 2018:188, 2018.\\n[4] Matthew BA McDermott, Tzu Ming Harry Hsu, Wei-Hung Weng, Marzyeh Ghassemi, and Peter Szolovits.\\nChexpert++: Approximating the chexpert labeler for speed, differentiability, and probabilistic output. In\\nMachine Learning for Healthcare Conference, pages 913–927. PMLR, 2020.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 10, 'page_label': '11', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='[5] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek, Andrew Y Ng, and Matthew P Lungren.\\nChexbert: combining automatic labelers and expert annotations for accurate radiology report labeling using\\nbert. arXiv preprint arXiv:2004.09167, 2020.\\n[6] Saahil Jain, Akshay Smit, Steven QH Truong, Chanh DT Nguyen, Minh-Thanh Huynh, Mudit Jain,\\nVictoria A Young, Andrew Y Ng, Matthew P Lungren, and Pranav Rajpurkar. Visualchexbert: addressing\\nthe discrepancy between radiology report labels and image labels. In Proceedings of the Conference on\\nHealth, Inference, and Learning, pages 105–115, 2021.\\n[7] Curtis P Langlotz and Lee Meininger. Enhancing the expressiveness and usability of structured image\\nreporting systems. In Proceedings of the AMIA symposium, page 467. American Medical Informatics\\nAssociation, 2000.\\n[8] Saeed Hassanpour and Curtis P Langlotz. Information extraction from multi-institutional radiology reports.\\nArtiﬁcial intelligence in medicine, 66:29–39, 2016.\\n[9] Surabhi Datta, Yuqi Si, Laritza Rodriguez, Sonya E Shooshan, Dina Demner-Fushman, and Kirk Roberts.\\nUnderstanding spatial language in radiology: representation framework, annotation, and spatial relation\\nextraction from chest x-ray reports using deep learning. Journal of biomedical informatics, 108:103473,\\n2020.\\n[10] Jackson M Steinkamp, Charles Chambers, Darco Lalevic, Hanna M Zafar, and Tessa S Cook. Toward\\ncomplete structured information extraction from radiology reports using machine learning. Journal of\\ndigital imaging, 32(4):554–564, 2019.\\n[11] Kento Sugimoto, Toshihiro Takeda, Jong-Hoon Oh, Shoya Wada, Shozo Konishi, Asuka Yamahata, Shiro\\nManabe, Noriyuki Tomiyama, Takashi Matsunaga, Katsuyuki Nakanishi, et al. Extracting clinical terms\\nfrom radiology reports with deep learning. Journal of Biomedical Informatics, 116:103729, 2021.\\n[12] Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C Kipper-\\nSchuler, and Christopher G Chute. Mayo clinical text analysis and knowledge extraction system (ctakes):\\narchitecture, component evaluation and applications. Journal of the American Medical Informatics\\nAssociation, 17(5):507–513, 2010.\\n[13] Sergey Goryachev, Margarita Sordo, and Qing T Zeng. A suite of natural language processing tools\\ndeveloped for the i2b2 project. In AMIA Annual Symposium Proceedings , volume 2006, page 931.\\nAmerican Medical Informatics Association, 2006.\\n[14] Alan R Aronson and François-Michel Lang. An overview of metamap: historical perspective and recent\\nadvances. Journal of the American Medical Informatics Association, 17(3):229–236, 2010.\\n[15] Surabhi Datta, Morgan Ulinski, Jordan Godfrey-Stovall, Shekhar Khanpara, Roy F Riascos-Castaneda, and\\nKirk Roberts. Rad-spatialnet: A frame-based resource for ﬁne-grained spatial relations in radiology reports.\\nIn LREC... International Conference on Language Resources & Evaluation:[proceedings]. International\\nConference on Language Resources and Evaluation, volume 2020, page 2251. NIH Public Access, 2020.\\n[16] Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria de la Iglesia-Vayá. Padchest: A large\\nchest x-ray image dataset with multi-label annotated reports. Medical image analysis, 66:101797, 2020.\\n[17] Surabhi Datta and Kirk Roberts. A dataset of chest x-ray reports annotated with spatial role labeling\\nannotations. Data in Brief, 32:106056, 2020.\\n[18] Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identiﬁcation of entities,\\nrelations, and coreference for scientiﬁc knowledge graph construction. arXiv preprint arXiv:1808.09602,\\n2018.\\n[19] Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum. SemEval\\n2017 task 10: ScienceIE - extracting keyphrases and relations from scientiﬁc publications. In Proceedings\\nof the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 546–555, Vancouver,\\nCanada, August 2017. Association for Computational Linguistics.\\n[20] Zexuan Zhong and Danqi Chen. A frustratingly easy approach for joint entity and relation extraction.\\nCoRR, abs/2010.12812, 2020.\\n[21] Min Song, Won Chul Kim, Dahee Lee, Go Eun Heo, and Keun Young Kang. Pkde4j: Entity and relation\\nextraction for public knowledge discovery. Journal of biomedical informatics, 57:320–332, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2021-08-31T00:52:19+00:00', 'author': '', 'keywords': '', 'moddate': '2021-08-31T00:52:19+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/RadGraph.pdf', 'total_pages': 12, 'page': 11, 'page_label': '12', 'source_file': 'RadGraph.pdf', 'file_type': 'pdf'}, page_content='[22] Lev Ratinov and Dan Roth. Design challenges and misconceptions in named entity recognition. In\\nProceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL-2009),\\npages 147–155, 2009.\\n[23] Erik F Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent\\nnamed entity recognition. arXiv preprint cs/0306050, 2003.\\n[24] Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. Kernel methods for relation extraction. Journal\\nof machine learning research, 3(Feb):1083–1106, 2003.\\n[25] Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. A general\\nframework for information extraction using dynamic span graphs. arXiv preprint arXiv:1904.03296, 2019.\\n[26] David Wadden, Ulme Wennberg, Yi Luan, and Hannaneh Hajishirzi. Entity, relation, and event extraction\\nwith contextualized span representations. CoRR, abs/1909.03546, 2019.\\n[27] Fei Li, Meishan Zhang, Guohong Fu, and Donghong Ji. A neural joint model for entity and relation\\nextraction from biomedical text. BMC bioinformatics, 18(1):1–11, 2017.\\n[28] Datasaur, Inc., Sunnyvale, California, United States. Datasaur, 2019.\\n[29] David Carrell, Bradley Malin, John Aberdeen, Samuel Bayer, Cheryl Clark, Ben Wellner, and Lynette\\nHirschman. Hiding in plain sight: use of realistic surrogates to reduce exposure of protected health\\ninformation in clinical text. Journal of the American Medical Informatics Association, 20(2):342–348,\\n2013.\\n[30] Laleh Seyyed-Kalantari, Guanxiong Liu, Matthew McDermott, Irene Y Chen, and Marzyeh Ghassemi.\\nChexclusion: Fairness gaps in deep chest x-ray classiﬁers. In BIOCOMPUTING 2021: Proceedings of the\\nPaciﬁc Symposium, pages 232–243. World Scientiﬁc, 2020.\\n[31] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[32] Shanchan Wu and Yifan He. Enriching pre-trained language model with entity information for relation\\nclassiﬁcation. In Proceedings of the 28th ACM International Conference on Information and Knowledge\\nManagement, pages 2361–2364, 2019.\\n[33] Lance A Ramshaw and Mitchell P Marcus. Text chunking using transformation-based learning. In Natural\\nlanguage processing using very large corpora, pages 157–176. Springer, 1999.\\n[34] Zexuan Zhong and Danqi Chen. A frustratingly easy approach for joint entity and relation extraction.\\narXiv preprint arXiv:2010.12812, 2020.\\n[35] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo\\nKang. BioBERT: a pre-trained biomedical language representation model for biomedical text mining.\\nBioinformatics, 36(4):1234–1240, 09 2019.\\n[36] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew\\nMcDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.\\n[37] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann,\\nJianfeng Gao, and Hoifung Poon. Domain-speciﬁc language model pretraining for biomedical natural\\nlanguage processing. arXiv preprint arXiv:2007.15779, 2020.\\n[38] Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in biomedical natural language processing:\\nAn evaluation of BERT and elmo on ten benchmarking datasets. CoRR, abs/1906.05474, 2019.\\n[39] Jean Carletta. Assessing agreement on classiﬁcation tasks: the kappa statistic. arXiv preprint cmp-\\nlg/9602004, 1996.\\n12')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc192129-1182-4ee2-9924-88cdb5b76d58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35860b8-e691-4041-be36-4a23f513bfd0",
   "metadata": {},
   "source": [
    "## Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c8c45e-d350-4121-9ebe-4e754f5452d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size = 1000, chunk_overlap=200):\n",
    "    \"\"\" Split the documents into chunks \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        # Paragraph, New Line, Space, Nothing\n",
    "        separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample_chunk\")\n",
    "        print(f\"Content 1: {split_docs[0].page_content[300:]}...\")\n",
    "        print(f\"Content 2: {split_docs[1].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}...\")\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c4fcaa-52ed-41b8-8525-c9906cabd790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 27 documents into 111 chunks\n",
      "\n",
      "Example_chunk\n",
      "Content 1: mar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions...\n",
      "Content 2: mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine transla...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}...\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "696af395-1a82-4ed2-8392-94edd6cc7a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'source_file': 'AttentionIsAllYouNeed.pdf', 'file_type': 'pdf'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7865a-726f-40f5-8e79-6e47f6d48d4c",
   "metadata": {},
   "source": [
    "#### As we can see the document got split into chunks. The length of each chunk is 500 and the first 200 words of the second chunk = last 200 words of the first chunk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea0f3c-f481-4866-8d53-7faea7363e23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49b4353e-5292-4b46-bf8c-c56da2d89932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Tuple, Any, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142b06a0-9b6c-49d3-b041-88216c14509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\" Handels document embedding generation using SentenceTransformer \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "            Init the Embedding Manager\n",
    "\n",
    "            Args:\n",
    "                Name of the pre-trained embedding model.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        # Having this here automatically runs the function as soon as we init a EmbeddingsManager\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\" Loads the SentenceTransformer model \"\"\"\n",
    "        try: \n",
    "            print(f\"Loading model {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model succesfully loaded with embedding dim: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Loading embeddings for texts\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Loaded embeddings for texts with dim: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "    def get_embeddings_dimension(self) -> int:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "\n",
    "        print(f\"Model embedding dimension is: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        return self.model.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17cc0242-befd-4edf-a11c-73758134dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d1a06be-2a56-427b-a6fb-0b98150bf094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model all-MiniLM-L6-v2\n",
      "Model succesfully loaded with embedding dim: 384\n",
      "Loading embeddings for texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837a7d36a3b642228dd4c5a7ad48d96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings for texts with dim: (111, 384)\n",
      "Model embedding dimension is: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embm1 = EmbeddingManager()\n",
    "embeddings = embm1.generate_embeddings(texts)\n",
    "embm1.get_embeddings_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bd3f5-fee5-4ca7-8893-46e8456873e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e900abc-5c7a-41d5-93cf-94458c64695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore: \n",
    "\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        try: \n",
    "            # If the directory exists, keep it, if not make one\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            # Client that has reference to the chromadb vectorstore\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            # Collection, what is in the collection?\n",
    "            # Collection is where we are storing the vectors inside of the vector store.\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name, \n",
    "                metadata = {\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured with initialization: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        document_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            document_text.append(doc.page_content)\n",
    "\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=document_text\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error with adding to vector store: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad98f14-5381-44d1-8396-b136f2657321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x17bcbff50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d5850f2-1eea-4507-919a-6b1363e9ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add everything to the vector database\n",
    "vectorstore.add_documents(documents=chunks, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c1151-4cf6-4ab6-b763-fc372c19522c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "331b321d-222f-41d9-b9f5-bfdf96ae328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embm1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61456a6-23b5-4ce5-b554-8cd33cdc8f6a",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f97d4b41-4df3-43ca-a8b2-966fbd1a48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is the mathematical formula for self attention?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Loading embeddings for texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a1ec0c1b244a02b1d4925208cc3e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings for texts with dim: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_61138e88_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'author': '',\n",
       "   'content_length': 216,\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 12,\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf',\n",
       "   'source_file': 'AttentionIsAllYouNeed.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 15,\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.17520487308502197,\n",
       "  'distance': 0.824795126914978,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_59f9d02d_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'author': '',\n",
       "   'content_length': 216,\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 12,\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf',\n",
       "   'source_file': 'AttentionIsAllYouNeed.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 15,\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.17520487308502197,\n",
       "  'distance': 0.824795126914978,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_337ebdb1_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'author': '',\n",
       "   'content_length': 216,\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 12,\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf',\n",
       "   'source_file': 'AttentionIsAllYouNeed.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 15,\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.17520487308502197,\n",
       "  'distance': 0.824795126914978,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_a0d2f273_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'author': '',\n",
       "   'content_length': 216,\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 12,\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf',\n",
       "   'source_file': 'AttentionIsAllYouNeed.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 15,\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.17520487308502197,\n",
       "  'distance': 0.824795126914978,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_ba83c63c_12',\n",
       "  'content': '3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3',\n",
       "  'metadata': {'author': '',\n",
       "   'content_length': 216,\n",
       "   'creationdate': '2024-04-10T21:11:43+00:00',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 12,\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'moddate': '2024-04-10T21:11:43+00:00',\n",
       "   'page': 2,\n",
       "   'page_label': '3',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source': '../data/pdf_files/AttentionIsAllYouNeed.pdf',\n",
       "   'source_file': 'AttentionIsAllYouNeed.pdf',\n",
       "   'subject': '',\n",
       "   'title': '',\n",
       "   'total_pages': 15,\n",
       "   'trapped': '/False'},\n",
       "  'similarity_score': 0.17520487308502197,\n",
       "  'distance': 0.824795126914978,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = rag_retriever.retrieve(\"What is the mathematical formula for self attention?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac14962-8a49-442f-9cd6-b75b5a20feca",
   "metadata": {},
   "source": [
    "# LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfe15df9-f196-42f1-8286-e641ba83f031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    device=-1\n",
    ")\n",
    "hf_llm = HuggingFacePipeline(pipeline=gen_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac15b819-1191-4aec-903e-24e6f1413b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_simple(query, retreiver, llm, device=\"cpu\", top_k=3):\n",
    "    results = retreiver.retrieve(query=query, top_k=top_k)\n",
    "    context = '\\n\\n'.join([r['content'] for r in results]) if results else \"\"\n",
    "    if len(context) == 0:\n",
    "        print(\"Context not long enough for this query\")\n",
    "        return\n",
    "\n",
    "    # Generate an answer using our prompt\n",
    "    simple_prompt = f\"\"\"\n",
    "        You are a math and deep learning expert. \n",
    "        Use the provided context to answer clearly and include equations when relevant.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "    llm_response = llm.invoke([simple_prompt.format(context=context, query=query)])\n",
    "    if \"Answer:\" in llm_response:\n",
    "        answer = llm_response.split(\"Answer:\", 1)[1]\n",
    "    else:\n",
    "        answer = llm_response\n",
    "    return \" \".join(answer.split())  # Cleans up unwanted characters from the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7215c4fe-1904-4652-9ef3-31d2177d13ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Loading embeddings for texts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec054607edd4c5e84967ab0b8f96321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings for texts with dim: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.2 Attention Attention is a function, which is used to compute a weighted sum of two vectors, where the dot product is a function of the scores of both vectors. 3 An attention function is an important part of neural networks, and it is used to process large amounts of data. It is used to recognize important information or patterns in data. For example, an attention function might be used to recognize if two sentences are similar or different, or whether two words belong together or not. The output of an attention function is a weighted sum of the scores of both vectors, which is then used to produce a prediction. The input and output of an attention function are a set of two vectors that are concatenated. This can be seen as a set of weights, which are used to compute the output of the attention function. The output of an attention function is a vector, which is the weighted sum of the scores of both vectors. An example of an attention function is the following code: ```python import torch def attention(scores, keys): \"\"\" Compute attention for a list of key-value pairs. :param scores: list of scores :param keys:'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is attention?\"\n",
    "answer = rag_simple(query, rag_retriever, hf_llm)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af223ad1-8978-40b1-acd6-bde60937b28a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
